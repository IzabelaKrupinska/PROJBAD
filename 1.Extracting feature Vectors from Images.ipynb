{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"},"colab":{"name":"1.Extracting feature Vectors from Images.ipynb","provenance":[{"file_id":"1SMsSLylNLhxaub0TUVbTIPW5VpT4YkkC","timestamp":1614186290823}],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"rJa3ZdLjNfG-"},"source":["tf_version = !pip3 show tensorflow | grep Version\n","if '2.2' not in tf_version[0]:\n","  print(\"downgrading tensorflow\")\n","  !pip3 uninstall --yes tensorflow\n","  !pip3 install tensorflow==2.2\n","\n","keras_version = !pip3 show keras | grep Version\n","if '2.3.1' not in keras_version[0]:\n","  print(\"downgrading keras\")\n","  !pip3 uninstall --yes keras\n","  !pip3 install keras==2.3.1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4IVSt4dBMBNA"},"source":["import numpy as np\n","from numpy import array\n","import pandas as pd\n","import matplotlib.pyplot as plt \n","import string\n","import os\n","from PIL import Image\n","import glob\n","from pickle import dump, load\n","from tqdm import tqdm_notebook as tqdm\n","from time import time\n","from keras.preprocessing import sequence\n","from keras.models import load_model\n","from keras.models import Sequential\n","from keras.layers import LSTM, Embedding, TimeDistributed, Dense, RepeatVector,\\\n","                         Activation, Flatten, Reshape, concatenate, Dropout, BatchNormalization\n","from keras.optimizers import Adam, RMSprop\n","from keras.layers.wrappers import Bidirectional\n","from keras.layers.merge import add\n","from keras.applications.inception_v3 import InceptionV3\n","from keras.preprocessing import image\n","from keras.models import Model\n","from keras import Input, layers\n","from keras import optimizers\n","from keras.applications.inception_v3 import preprocess_input\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","\n","try:\n","    import dill as pickle\n","except ImportError:\n","    import pickle"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Hg1biHM8OgY_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614199532586,"user_tz":-60,"elapsed":17843,"user":{"displayName":"Bany Krupińska","photoUrl":"","userId":"09002400337408493023"}},"outputId":"f02111ee-19c5-43ac-89ff-8f55eb6deeeb"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"WtVS4-coMBNb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614199534228,"user_tz":-60,"elapsed":1638,"user":{"displayName":"Bany Krupińska","photoUrl":"","userId":"09002400337408493023"}},"outputId":"48b32829-d874-4e43-9b67-eb0ceea662a4"},"source":["# load doc into memory\n","def load_doc(filename): \n","    file = open(filename, 'r') \n","    text = file.read() \n","    file.close()\n","    return text\n","\n","base_dir = \"/content/drive/My Drive/MB/\"\n","dataset_dir = \"/content/drive/My Drive/\"\n","descriptions_filename = dataset_dir + \"Flickr8k_text/Flickr8k.token.txt\"\n","# load descriptions\n","descriptions_doc = load_doc(descriptions_filename)\n","print(descriptions_doc[:300])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["1000268201_693b08cb0e.jpg#0\tA child in a pink dress is climbing up a set of stairs in an entry way .\n","1000268201_693b08cb0e.jpg#1\tA girl going into a wooden building .\n","1000268201_693b08cb0e.jpg#2\tA little girl climbing into a wooden playhouse .\n","1000268201_693b08cb0e.jpg#3\tA little girl climbing the s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"HRMoeUOaMBNk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614199715637,"user_tz":-60,"elapsed":966,"user":{"displayName":"Bany Krupińska","photoUrl":"","userId":"09002400337408493023"}},"outputId":"4d17dc2a-8fe9-437b-9880-4ff5db1c3979"},"source":["def load_descriptions(doc):\n","    mapping = dict()\n","    \n","    for line in doc.split('\\n'):\n","        \n","        if len(line) < 2:\n","            continue\n","        tokens = line.split()\n","        \n","        image_id, image_desc = tokens[0], tokens[1:]\n","        \n","        image_id = image_id.split('.')[0]\n","        \n","        image_desc = ' '.join(image_desc)\n","        \n","        if image_id not in mapping:\n","            mapping[image_id] = list()\n","        \n","        mapping[image_id].append(image_desc)\n","    return mapping\n","\n","descriptions = load_descriptions(descriptions_doc)\n","print('Loaded: %d ' % len(descriptions))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Loaded: 8092 \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qCgh0IKeMBNq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614199717493,"user_tz":-60,"elapsed":711,"user":{"displayName":"Bany Krupińska","photoUrl":"","userId":"09002400337408493023"}},"outputId":"f55edd1f-296d-4f9d-c4cc-753b1fab1b6b"},"source":["list(descriptions.items())[:1]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('1000268201_693b08cb0e',\n","  ['A child in a pink dress is climbing up a set of stairs in an entry way .',\n","   'A girl going into a wooden building .',\n","   'A little girl climbing into a wooden playhouse .',\n","   'A little girl climbing the stairs to her playhouse .',\n","   'A little girl in a pink dress going into a wooden cabin .'])]"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"KJZjASiGMBN0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614199808043,"user_tz":-60,"elapsed":67151,"user":{"displayName":"Bany Krupińska","photoUrl":"","userId":"09002400337408493023"}},"outputId":"990a7c49-59bf-44d9-f8a0-dec6d9ced004"},"source":["from nltk.tokenize import RegexpTokenizer\n","from nltk.corpus import stopwords\n","from nltk.stem.wordnet import WordNetLemmatizer\n","\n","import nltk\n","nltk.download('all')\n","\n","tokenizer = RegexpTokenizer(r'\\w+')\n","\n","def clean_descriptions(descriptions):\n","    \n","    lemma=WordNetLemmatizer()\n","    stop_words = set(stopwords.words('english'))\n","    table = str.maketrans('', '', string.punctuation)\n","    for key, desc_list in descriptions.items():\n","        for i in range(len(desc_list)):\n","            desc = desc_list[i]\n","            \n","            # create tokens from sentence\n","            desc = tokenizer.tokenize(desc)\n","            \n","            # make tokens lowercase\n","            desc = [word.lower() for word in desc]\n","            \n","            # filter one letter tokens\n","            desc = [word for word in desc if len(word)>1]\n","\n","            # filter stop words ('or', 'because', 'as', ...)\n","            desc = [word for word in desc if not word in stop_words]\n","            \n","            # lemmatize words \n","            desc = [lemma.lemmatize(word) for word in desc]\n","            \n","            # merge into one string\n","            desc_list[i] =  ' '.join(desc)\n","\n","clean_descriptions(descriptions)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading collection 'all'\n","[nltk_data]    | \n","[nltk_data]    | Downloading package abc to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/abc.zip.\n","[nltk_data]    | Downloading package alpino to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/alpino.zip.\n","[nltk_data]    | Downloading package biocreative_ppi to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n","[nltk_data]    | Downloading package brown to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/brown.zip.\n","[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n","[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n","[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n","[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/chat80.zip.\n","[nltk_data]    | Downloading package city_database to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/city_database.zip.\n","[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/cmudict.zip.\n","[nltk_data]    | Downloading package comparative_sentences to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n","[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n","[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/conll2000.zip.\n","[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/conll2002.zip.\n","[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n","[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/crubadan.zip.\n","[nltk_data]    | Downloading package dependency_treebank to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n","[nltk_data]    | Downloading package dolch to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/dolch.zip.\n","[nltk_data]    | Downloading package europarl_raw to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n","[nltk_data]    | Downloading package floresta to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/floresta.zip.\n","[nltk_data]    | Downloading package framenet_v15 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n","[nltk_data]    | Downloading package framenet_v17 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n","[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n","[nltk_data]    | Downloading package genesis to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/genesis.zip.\n","[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n","[nltk_data]    | Downloading package ieer to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/ieer.zip.\n","[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/inaugural.zip.\n","[nltk_data]    | Downloading package indian to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/indian.zip.\n","[nltk_data]    | Downloading package jeita to /root/nltk_data...\n","[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/kimmo.zip.\n","[nltk_data]    | Downloading package knbc to /root/nltk_data...\n","[nltk_data]    | Downloading package lin_thesaurus to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n","[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n","[nltk_data]    | Downloading package machado to /root/nltk_data...\n","[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n","[nltk_data]    | Downloading package moses_sample to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping models/moses_sample.zip.\n","[nltk_data]    | Downloading package movie_reviews to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n","[nltk_data]    | Downloading package names to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/names.zip.\n","[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n","[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n","[nltk_data]    | Downloading package omw to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/omw.zip.\n","[nltk_data]    | Downloading package opinion_lexicon to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n","[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/paradigms.zip.\n","[nltk_data]    | Downloading package pil to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/pil.zip.\n","[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/pl196x.zip.\n","[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/ppattach.zip.\n","[nltk_data]    | Downloading package problem_reports to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n","[nltk_data]    | Downloading package propbank to /root/nltk_data...\n","[nltk_data]    | Downloading package ptb to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/ptb.zip.\n","[nltk_data]    | Downloading package product_reviews_1 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n","[nltk_data]    | Downloading package product_reviews_2 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n","[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n","[nltk_data]    | Downloading package qc to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/qc.zip.\n","[nltk_data]    | Downloading package reuters to /root/nltk_data...\n","[nltk_data]    | Downloading package rte to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/rte.zip.\n","[nltk_data]    | Downloading package semcor to /root/nltk_data...\n","[nltk_data]    | Downloading package senseval to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/senseval.zip.\n","[nltk_data]    | Downloading package sentiwordnet to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n","[nltk_data]    | Downloading package sentence_polarity to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n","[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n","[nltk_data]    | Downloading package sinica_treebank to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n","[nltk_data]    | Downloading package smultron to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/smultron.zip.\n","[nltk_data]    | Downloading package state_union to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/state_union.zip.\n","[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n","[nltk_data]    |   Package stopwords is already up-to-date!\n","[nltk_data]    | Downloading package subjectivity to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n","[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/swadesh.zip.\n","[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/switchboard.zip.\n","[nltk_data]    | Downloading package timit to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/timit.zip.\n","[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/toolbox.zip.\n","[nltk_data]    | Downloading package treebank to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/treebank.zip.\n","[nltk_data]    | Downloading package twitter_samples to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n","[nltk_data]    | Downloading package udhr to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/udhr.zip.\n","[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/udhr2.zip.\n","[nltk_data]    | Downloading package unicode_samples to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n","[nltk_data]    | Downloading package universal_treebanks_v20 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/verbnet.zip.\n","[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n","[nltk_data]    | Downloading package webtext to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/webtext.zip.\n","[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/wordnet.zip.\n","[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n","[nltk_data]    | Downloading package words to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/words.zip.\n","[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/ycoe.zip.\n","[nltk_data]    | Downloading package rslp to /root/nltk_data...\n","[nltk_data]    |   Unzipping stemmers/rslp.zip.\n","[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n","[nltk_data]    | Downloading package universal_tagset to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n","[nltk_data]    | Downloading package maxent_ne_chunker to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n","[nltk_data]    | Downloading package punkt to /root/nltk_data...\n","[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n","[nltk_data]    | Downloading package book_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n","[nltk_data]    | Downloading package sample_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n","[nltk_data]    | Downloading package spanish_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n","[nltk_data]    | Downloading package basque_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n","[nltk_data]    | Downloading package large_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n","[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n","[nltk_data]    |   Unzipping help/tagsets.zip.\n","[nltk_data]    | Downloading package snowball_data to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    | Downloading package bllip_wsj_no_aux to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n","[nltk_data]    | Downloading package word2vec_sample to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n","[nltk_data]    | Downloading package panlex_swadesh to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n","[nltk_data]    | Downloading package averaged_perceptron_tagger to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n","[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping\n","[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n","[nltk_data]    | Downloading package perluniprops to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping misc/perluniprops.zip.\n","[nltk_data]    | Downloading package nonbreaking_prefixes to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n","[nltk_data]    | Downloading package vader_lexicon to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n","[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n","[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n","[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n","[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n","[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n","[nltk_data]    | \n","[nltk_data]  Done downloading collection all\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"yRbjjF-WMBN7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614199960085,"user_tz":-60,"elapsed":716,"user":{"displayName":"Bany Krupińska","photoUrl":"","userId":"09002400337408493023"}},"outputId":"3672f306-98a3-44d9-f260-43a31e0c63ae"},"source":["descriptions['1000268201_693b08cb0e']"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['child pink dress climbing set stair entry way',\n"," 'girl going wooden building',\n"," 'little girl climbing wooden playhouse',\n"," 'little girl climbing stair playhouse',\n"," 'little girl pink dress going wooden cabin']"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"code","metadata":{"id":"arzjE4wXMBOB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614200131848,"user_tz":-60,"elapsed":730,"user":{"displayName":"Bany Krupińska","photoUrl":"","userId":"09002400337408493023"}},"outputId":"44e811a8-1dbb-484b-a405-c61d3f5da70b"},"source":["# convert the loaded descriptions into a vocabulary of words\n","def getVocabulary(descriptions):\n","    # build a list of all description strings\n","    all_desc = set()\n","    for key in descriptions.keys():\n","        [all_desc.update(tokenizer.tokenize(d)) for d in descriptions[key]]\n","    return all_desc\n","\n","# summarize vocabulary\n","vocabulary = getVocabulary(descriptions)\n","print('Original Vocabulary Size: %d' % len(vocabulary))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Original Vocabulary Size: 7146\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lorDD-YkMBOF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614200134088,"user_tz":-60,"elapsed":1273,"user":{"displayName":"Bany Krupińska","photoUrl":"","userId":"09002400337408493023"}},"outputId":"f669a060-925d-4e26-d85a-ef4842ddb054"},"source":["# save descriptions to file, one per line\n","def save_descriptions(descriptions, filename):\n","    lines = list()\n","    for key, desc_list in descriptions.items():\n","        for desc in desc_list:\n","            lines.append(key + ' ' + desc)\n","    data = '\\n'.join(lines)\n","    file = open(filename, 'w')\n","    file.write(data)\n","    file.close()\n","\n","save_descriptions(descriptions, base_dir + 'descriptions.txt')\n","print(\"Desciption.txt created\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Desciption.txt created\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"YjixZyZwVC4Y"},"source":["Preparing data for training"]},{"cell_type":"code","metadata":{"id":"Jm1jO0joMBOL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614200169063,"user_tz":-60,"elapsed":697,"user":{"displayName":"Bany Krupińska","photoUrl":"","userId":"09002400337408493023"}},"outputId":"0b6af227-a3d1-4af5-f711-aff4681cdfa9"},"source":["def load_set(filename):\n","    doc = load_doc(filename)\n","    dataset = list()\n","    \n","    for line in doc.split('\\n'):\n","        \n","        if len(line) < 1:\n","            continue\n"," \n","        dataset.append(line)\n","    return (dataset)\n"," \n","filename = dataset_dir + 'Flickr8k_text/Flickr_8k.trainImages.txt'\n","train_names = load_set(filename)\n","print('Train size: %d' % len(train_names))\n","\n","Directroy_path = dataset_dir + 'Flickr8k_Dataset/Flicker8k_Dataset/'\n","train_img= [Directroy_path+i  for i in train_names]"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Train size: 6000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"B4hCE0VdMBOS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614200171760,"user_tz":-60,"elapsed":693,"user":{"displayName":"Bany Krupińska","photoUrl":"","userId":"09002400337408493023"}},"outputId":"c017505d-ba4c-41ea-ae1d-5ad7f116e763"},"source":["filename = dataset_dir + 'Flickr8k_text/Flickr_8k.testImages.txt'\n","test_names = load_set(filename)\n","print('Test Size: %d' % len(test_names))\n","\n","Directroy_path = dataset_dir + 'Flickr8k_Dataset/Flicker8k_Dataset/'\n","test_img= [Directroy_path+i  for i in test_names]"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Test Size: 1000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"MUFWvIoBMBOY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614200176425,"user_tz":-60,"elapsed":3547,"user":{"displayName":"Bany Krupińska","photoUrl":"","userId":"09002400337408493023"}},"outputId":"bc377029-4c49-42b5-88d2-d88682b76e02"},"source":["# we r extracting train data captions such that, each captions starts with 'startseq' and ends with 'endseq\n","def load_clean_descriptions(filename, dataset):\n","    # load document\n","    dataset = [i.split('.')[0] for i in dataset]\n","    doc = load_doc(filename)\n","    descriptions = dict()\n","    for line in doc.split('\\n'):\n","        \n","        tokens = line.split()\n","        \n","        image_id, image_desc = tokens[0], tokens[1:]\n","        \n","        if image_id in dataset:\n","            \n","            if image_id not in descriptions:\n","                descriptions[image_id] = list()\n","            \n","            desc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n","            \n","            descriptions[image_id].append(desc)\n","    return descriptions\n","\n","# descriptions\n","train_descriptions = load_clean_descriptions(base_dir + 'descriptions.txt', train_names)\n","\n","print('Descriptions: train=%d' % len(train_descriptions))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Descriptions: train=6000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7p1f5m98cwP-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614200184135,"user_tz":-60,"elapsed":1426,"user":{"displayName":"Bany Krupińska","photoUrl":"","userId":"09002400337408493023"}},"outputId":"5228de1e-5b57-4ea4-c2fa-bd70a6bd736e"},"source":["test_descriptions = load_clean_descriptions(base_dir + 'descriptions.txt', test_names)\n","print('Descriptions: test=%d' % len(test_descriptions))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Descriptions: test=1000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RwScfkIyMBOe"},"source":["# Save train_descriptions\n","with open(base_dir + \"train_descriptions.pkl\", \"wb\") as encoded_pickle:\n","    pickle.dump(train_descriptions, encoded_pickle)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1gzF-sRxqfdT"},"source":["# Save test_descriptions\n","with open(base_dir + \"test_descriptions.pkl\", \"wb\") as encoded_pickle:\n","    pickle.dump(test_descriptions, encoded_pickle)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9xf27bqGVenc"},"source":["Extracting feature Vectors from Images"]},{"cell_type":"code","metadata":{"id":"likzFZnCMBOl"},"source":["def preprocess(image_path):\n","    # Convert all the images to size 299x299 as expected by the inception v3 model\n","    img = image.load_img(image_path, target_size=(299, 299))\n","    \n","    x = image.img_to_array(img)\n","    # Add one more dimension\n","    x = np.expand_dims(x, axis=0)\n","    \n","    x = preprocess_input(x)\n","    return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UzicWb2yMBOq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606517061648,"user_tz":-60,"elapsed":8706,"user":{"displayName":"Bany Krupińska","photoUrl":"","userId":"09002400337408493023"}},"outputId":"bd704416-17f4-4151-fb6d-86d5e5155639"},"source":["# Load the inception v3 model\n","model = InceptionV3(weights='imagenet')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.5/inception_v3_weights_tf_dim_ordering_tf_kernels.h5\n","96116736/96112376 [==============================] - 1s 0us/step\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"BKfM1FflMBOv"},"source":["# model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"URo9BwbXMBO1"},"source":["# Create a new model, by removing the last layer (output layer) from the inception v3\n","model_new = Model(model.input, model.layers[-2].output)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rjshJLS9MBO4"},"source":["# Function to encode a given image into a vector of size (2048, )\n","def encode(image):\n","    image = preprocess(image) \n","    v = model_new.predict(image) \n","    v = np.reshape(v, v.shape[1]) \n","    return v"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SZuI4F6cMBO8"},"source":["# # Encoding all the Training Dataset to a 2048 Dimension Vectors, Run this once\n","# start = time()\n","# k=0\n","# encoded_train = {}\n","# for img in tqdm(train_img):\n"," \n","#     encoded_train[img[len(Directroy_path):]] = encode(img)\n","# print(\"Time taken in seconds =\", time()-start)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QN7KGHmSMBPB"},"source":["# # Encoding all the Test Dataset to a 2048 Dimension Vectors, Run this once\n","# start = time()\n","# encoded_test = {}\n","# for img in tqdm(test_img):\n","#     encoded_test[img[len(Directroy_path):]] = encode(img)\n","# print(\"Time taken in seconds =\", time()-start)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Yke30bc1MBPE"},"source":["# # Save Train features\n"," with open(base_dir + \"encoded_train_images.pkl\", \"wb\") as encoded_pickle:\n","     pickle.dump(encoded_train, encoded_pickle)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h5_h-LLaMBPK"},"source":["# # Save test features \n","\n"," from pickle import dump, load\n"," with open(base_dir + \"encoded_test_images.pkl\", \"wb\") as encoded_pickle:\n","     pickle.dump(encoded_test, encoded_pickle)"],"execution_count":null,"outputs":[]}]}